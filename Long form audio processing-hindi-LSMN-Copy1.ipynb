{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f07be3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff1d35",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e522a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import librosa\n",
    "# import soundfile as sf\n",
    "\n",
    "# # Assuming 'output_filename' is the desired filename for the output WAV file\n",
    "# output_filename = 'long_form_audio/concatenated_output.wav'\n",
    "# data=pd.read_csv('../MADHAV_task_1/Task3/metadata_test_clean.tsv', sep='\\t', header=None, names=['path', 'text', 'num','sr'])\n",
    "# # List to store audio chunks\n",
    "# audio_chunks = []\n",
    "# target=\"\"\n",
    "# # Iterate over all files in the directory\n",
    "# for i in range(10):\n",
    "#     file_path = f\"output/audio_noise{i}.wav\"\n",
    "\n",
    "#     # Load the audio file\n",
    "#     audio, sr = librosa.load(file_path, sr=None)\n",
    "#     if(target==\"\"):\n",
    "#         target=data['text'][i]\n",
    "#     else:\n",
    "#         target+=\" \"+data['text'][i]\n",
    "#     # Append the audio to the list\n",
    "#     audio_chunks.append(audio)\n",
    "\n",
    "# # Concatenate all audio chunks\n",
    "# concatenated_audio = np.concatenate(audio_chunks)\n",
    "\n",
    "# # Save the concatenated audio to a new WAV file\n",
    "# sf.write(output_filename, concatenated_audio, samplerate=sr)\n",
    "# print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671464f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/suryansh/MADHAV\n"
     ]
    }
   ],
   "source": [
    "%cd /home/suryansh/MADHAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6ae81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt_file_path = \"./hindi_data/givenFolderHSp/txt/12janA.txt\"\n",
    "# file=open(\"./hindi_data/givenFolderHSp/txt/12janA.txt\", 'r')\n",
    "# target=file.readlines()\n",
    "# target=re.sub(\" +\", \" \", \" \".join(target))\n",
    "# # Now 'hindi_text' contains the contents of the file as a string\n",
    "# print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c411fdb",
   "metadata": {},
   "source": [
    "## Model algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2074f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_using_VAD(input_file, pred, chunk_length_sec=15):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(input_file, sr=None)\n",
    "    timestamps_df=pd.DataFrame()\n",
    "    timestamps_df['start']=pred['start'].apply(lambda x: x*sr/1000)\n",
    "    timestamps_df['end']=pred['end'].apply(lambda x: x*sr/1000)\n",
    "    print(timestamps_df)\n",
    "    # Calculate the total duration of the audio in samples\n",
    "    total_samples = len(audio)\n",
    "\n",
    "    # Calculate the chunk length in samples\n",
    "    chunk_length_samples = int(sr * chunk_length_sec)\n",
    "    print(chunk_length_samples)\n",
    "    # Create an array to store audio chunks\n",
    "    audio_chunks = []\n",
    "    start_sample=0\n",
    "    end_sample=chunk_length_samples\n",
    "    split_sec=[]\n",
    "    k=0\n",
    "    index=0\n",
    "    for i, row in timestamps_df.iterrows():\n",
    "        if (start_sample+chunk_length_samples)>=row['end']:\n",
    "            end_sample=row['end']\n",
    "        else:\n",
    "            audio_chunks.append(audio[int(start_sample):int(end_sample)])\n",
    "            split_sec.append((index, int(start_sample)/sr, int(end_sample)/sr))\n",
    "            index+=1\n",
    "            start_sample=end_sample\n",
    "            end_sample=row['end']\n",
    "            while(start_sample+chunk_length_samples<row['end']):\n",
    "                end_sample=start_sample+chunk_length_samples\n",
    "                audio_chunks.append(audio[int(start_sample):int(end_sample)])\n",
    "                split_sec.append((index, int(start_sample)/sr, int(end_sample)/sr))\n",
    "                start_sample=end_sample\n",
    "                index+=1\n",
    "                end_sample=row['end']\n",
    "            \n",
    "\n",
    "    last_chunk = audio[int(start_sample):int(total_samples)]\n",
    "    audio_chunks.append(last_chunk)\n",
    "    split_sec.append((index, start_sample/sr, total_samples/sr))\n",
    "    split_sec=pd.DataFrame(split_sec, columns=['index', 'start', 'end'])\n",
    "    return audio_chunks, split_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eafc0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fsmnvad import FSMNVad\n",
    "# # Specify the input audio file\n",
    "# input_file = \"./hindi_data/givenPMS/newGivenPMS/longAudio/april_v.wav\"\n",
    "# vad = FSMNVad()\n",
    "# segments = vad.segments_offline(input_file)\n",
    "# df=pd.DataFrame(segments, columns=['start', 'end'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56f3742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U funasr_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f6a01f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git lfs install\n",
    "# !git clone https://huggingface.co/funasr/FSMN-VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32b34b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from funasr_onnx import Fsmn_vad\n",
    "\n",
    "# model_dir = \"./FSMN-VAD\"\n",
    "# model = Fsmn_vad(model_dir, quantize=True)\n",
    "\n",
    "# input_file = \"./hindi_data/givenFolderHSp/fullAudio/12janA.wav\"\n",
    "\n",
    "# result = model(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af5a1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = np.asarray(result, dtype=np.int32)\n",
    "# result=result.reshape((result.shape[1],2))\n",
    "# df=pd.DataFrame(result, columns=['start', 'end'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "217e4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the audio into 15-second chunks with adjustment for the last chunk\n",
    "# chunks_array, split_sec = split_audio_using_VAD(input_file, df, 7)\n",
    "\n",
    "# # Now, chunks_array contains the audio chunks as numpy arrays\n",
    "# for i, chunk in enumerate(chunks_array):\n",
    "#     print(f\"Chunk {i}: start: {split_sec['start'][i]} end: {split_sec['end'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7054add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q espnet_model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab43d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@title Choose English ASR model { run: \"auto\" }\n",
    "\n",
    "# lang = 'en'\n",
    "# fs = 16000 #@param {type:\"integer\"}\n",
    "# tag = 'viks66/asr_train_asr_raw_hindi_bpe500' #@param [\"Shinji Watanabe/spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid.acc.ave\", \"kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave\"] {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4939cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git lfs install\n",
    "# !git clone https://huggingface.co/viks66/asr_train_asr_raw_hindi_bpe500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f6dfd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/suryansh/MADHAV/asr_train_asr_raw_hindi_bpe500\n"
     ]
    }
   ],
   "source": [
    "%cd /home/suryansh/MADHAV/asr_train_asr_raw_hindi_bpe500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6136083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import string\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "from espnet2.bin.asr_inference import Speech2Text\n",
    "\n",
    "\n",
    "d = ModelDownloader()\n",
    "# It may takes a while to download and build models\n",
    "speech2text = Speech2Text(\n",
    "    \"exp/asr_train_asr_raw_hindi_bpe500/config.yaml\",\n",
    "    \"exp/asr_train_asr_raw_hindi_bpe500/valid.acc.ave_10best.pth\",\n",
    "    device=\"cuda\",\n",
    "    minlenratio=0.0,\n",
    "    maxlenratio=0.5,\n",
    "    ctc_weight=0.3,\n",
    "    beam_size=10,\n",
    "    batch_size=0,\n",
    "    nbest=1\n",
    ")\n",
    "\n",
    "def text_normalizer(text):\n",
    "    text = text.upper()\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dddd5883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import librosa.display\n",
    "# from IPython.display import display, Audio\n",
    "# import matplotlib.pyplot as plt\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "\n",
    "# col_names = ['path', 'text', 'time', 'sr']\n",
    "# preds = \"\"\n",
    "# target = \"\"\n",
    "\n",
    "# for i, chunk in enumerate(chunks_array):\n",
    "#     start_time = time.time()\n",
    "#     speech = np.array([])\n",
    "#     duration=split_sec['end'][i]*1000-split_sec['start'][i] * 1000\n",
    "#     for _, row in df.iterrows():\n",
    "#         start_sample = row['start']-split_sec['start'][i] * 1000\n",
    "#         end_sample = row['end']-split_sec['start'][i] * 1000\n",
    "# #         print(start_sample, end_sample, duration)\n",
    "#         if(start_sample<0 and end_sample<0):\n",
    "#             continue\n",
    "#         if(start_sample>duration and end_sample>duration):\n",
    "#             break\n",
    "# #         print(\"Y\")\n",
    "#         speech = np.concatenate([speech, chunk[int(max(0,start_sample))*16:int(min(duration, end_sample))*16]])\n",
    "\n",
    "#     print(len(speech))\n",
    "#     nbests = speech2text(speech)\n",
    "#     if len(speech) != 0:\n",
    "# #         nbests = speech2text(speech)\n",
    "#         text, *_ = nbests[0]\n",
    "# #         output_filename = f'results/output_chunk{i}.wav'\n",
    "# #         sf.write(output_filename, speech, samplerate=16000)\n",
    "#         if(preds==\"\"):\n",
    "#             preds=(text_normalizer(text))\n",
    "#         else:\n",
    "#             preds += \" \" + (text_normalizer(text))\n",
    "\n",
    "#     print(i, \"/500\")\n",
    "#     elapsed_time = time.time() - start_time\n",
    "#     print(f\"Time taken: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e67bcf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "ground_truth=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65443838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/suryansh/MADHAV\n",
      "        start        end\n",
      "0     12960.0    58400.0\n",
      "1     66080.0   628480.0\n",
      "2    638240.0  1149440.0\n",
      "3   1153920.0  2094720.0\n",
      "4   2131200.0  3091360.0\n",
      "5   3091360.0  3311360.0\n",
      "6   3322080.0  3880160.0\n",
      "7   3890080.0  4378080.0\n",
      "8   4420800.0  4912640.0\n",
      "9   4960480.0  5438080.0\n",
      "10  5471680.0  5711040.0\n",
      "11  5728800.0  6532160.0\n",
      "12  6548000.0  7115360.0\n",
      "13  7158400.0  7167680.0\n",
      "14  7174400.0  7357440.0\n",
      "15  7361920.0  7710560.0\n",
      "16  7721600.0  8578560.0\n",
      "17  8586080.0  8839680.0\n",
      "18  8844960.0  8981920.0\n",
      "19  8986400.0  9179200.0\n",
      "20  9185760.0  9406880.0\n",
      "240000\n",
      "/home/suryansh/MADHAV/asr_train_asr_raw_hindi_bpe500\n",
      "45440\n",
      "0 / 49\n",
      "Time taken: 2.60 seconds\n",
      "232320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suryansh/anaconda3/lib/python3.11/site-packages/espnet/nets/ctc_prefix_score.py:323: RuntimeWarning: invalid value encountered in logaddexp\n",
      "  r_sum = self.xp.logaddexp(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 49\n",
      "Time taken: 14.63 seconds\n",
      "240000\n",
      "2 / 49\n",
      "Time taken: 14.06 seconds\n",
      "90080\n",
      "3 / 49\n",
      "Time taken: 4.93 seconds\n",
      "230240\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#     nbests = speech2text(chunk)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(speech) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m         nbests \u001b[38;5;241m=\u001b[39m speech2text(speech)\n\u001b[1;32m     53\u001b[0m         text, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m nbests[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#         output_filename = f'results/output_chunk{i}.wav'\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#         sf.write(output_filename, speech, samplerate=16000)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet2/bin/asr_inference.py:530\u001b[0m, in \u001b[0;36mSpeech2Text.__call__\u001b[0;34m(self, speech)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(enc) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(enc)\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# c. Passed the encoder result and the beam search\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_single_sample(enc[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Encoder intermediate CTC predictions\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m intermediate_outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet2/bin/asr_inference.py:623\u001b[0m, in \u001b[0;36mSpeech2Text._decode_single_sample\u001b[0;34m(self, enc)\u001b[0m\n\u001b[1;32m    621\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetup_step\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    622\u001b[0m                     module\u001b[38;5;241m.\u001b[39msetup_step()\n\u001b[0;32m--> 623\u001b[0m     nbest_hyps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeam_search(\n\u001b[1;32m    624\u001b[0m         x\u001b[38;5;241m=\u001b[39menc, maxlenratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxlenratio, minlenratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminlenratio\n\u001b[1;32m    625\u001b[0m     )\n\u001b[1;32m    627\u001b[0m nbest_hyps \u001b[38;5;241m=\u001b[39m nbest_hyps[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbest]\n\u001b[1;32m    629\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet/nets/beam_search.py:433\u001b[0m, in \u001b[0;36mBeamSearch.forward\u001b[0;34m(self, x, maxlenratio, minlenratio, pre_x)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(maxlen):\n\u001b[1;32m    432\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i))\n\u001b[0;32m--> 433\u001b[0m     best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch(running_hyps, x, pre_x\u001b[38;5;241m=\u001b[39mpre_x)\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;66;03m# post process of one iteration\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     running_hyps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_process(i, maxlen, maxlenratio, best, ended_hyps)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet/nets/beam_search.py:339\u001b[0m, in \u001b[0;36mBeamSearch.search\u001b[0;34m(self, running_hyps, x, pre_x)\u001b[0m\n\u001b[1;32m    337\u001b[0m     hs, scores, states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_full(hyp, x, pre_x\u001b[38;5;241m=\u001b[39mpre_x)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     scores, states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_full(hyp, x, pre_x\u001b[38;5;241m=\u001b[39mpre_x)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_scorers:\n\u001b[1;32m    341\u001b[0m     weighted_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[k] \u001b[38;5;241m*\u001b[39m scores[k]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet/nets/beam_search.py:200\u001b[0m, in \u001b[0;36mBeamSearch.score_full\u001b[0;34m(self, hyp, x, pre_x)\u001b[0m\n\u001b[1;32m    198\u001b[0m         scores[k], states[k] \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mscore(hyp\u001b[38;5;241m.\u001b[39myseq, hyp\u001b[38;5;241m.\u001b[39mstates[k], x, pre_x)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         scores[k], states[k] \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mscore(hyp\u001b[38;5;241m.\u001b[39myseq, hyp\u001b[38;5;241m.\u001b[39mstates[k], x)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_hs:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hs, scores, states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet2/asr/decoder/transformer_decoder.py:222\u001b[0m, in \u001b[0;36mBaseTransformerDecoder.score\u001b[0;34m(self, ys, state, x, return_hs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logp\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), hs, state\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     logp, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_one_step(\n\u001b[1;32m    223\u001b[0m         ys\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    224\u001b[0m         ys_mask,\n\u001b[1;32m    225\u001b[0m         x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    226\u001b[0m         cache\u001b[38;5;241m=\u001b[39mstate,\n\u001b[1;32m    227\u001b[0m         return_hs\u001b[38;5;241m=\u001b[39mreturn_hs,\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logp\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), state\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet2/asr/decoder/transformer_decoder.py:191\u001b[0m, in \u001b[0;36mBaseTransformerDecoder.forward_one_step\u001b[0;34m(self, tgt, tgt_mask, memory, cache, return_hs)\u001b[0m\n\u001b[1;32m    189\u001b[0m new_cache \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c, decoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(cache, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders):\n\u001b[0;32m--> 191\u001b[0m     x, tgt_mask, memory, memory_mask \u001b[38;5;241m=\u001b[39m decoder(\n\u001b[1;32m    192\u001b[0m         x, tgt_mask, memory, \u001b[38;5;28;01mNone\u001b[39;00m, cache\u001b[38;5;241m=\u001b[39mc\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m     new_cache\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_before:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet/nets/pytorch_backend/transformer/decoder_layer.py:126\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, tgt, tgt_mask, memory, memory_mask, cache, pre_memory, pre_memory_mask)\u001b[0m\n\u001b[1;32m    124\u001b[0m     x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat_linear1(tgt_concat)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(tgt_q, tgt, tgt, tgt_q_mask))\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_before:\n\u001b[1;32m    128\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet/nets/pytorch_backend/transformer/attention.py:111\u001b[0m, in \u001b[0;36mMultiHeadedAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m    109\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_qkv(query, key, value)\n\u001b[1;32m    110\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_attention(v, scores, mask)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet/nets/pytorch_backend/transformer/attention.py:81\u001b[0m, in \u001b[0;36mMultiHeadedAttention.forward_attention\u001b[0;34m(self, value, scores, mask)\u001b[0m\n\u001b[1;32m     79\u001b[0m     min_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(scores\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m     80\u001b[0m     scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(mask, min_value)\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmasked_fill(\n\u001b[1;32m     82\u001b[0m         mask, \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     83\u001b[0m     )  \u001b[38;5;66;03m# (batch, head, time1, time2)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch, head, time1, time2)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import librosa.display\n",
    "from IPython.display import display, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from funasr_onnx import Fsmn_vad\n",
    "files=['./hindi_data/givenFolderHSp/fullAudio/12janA.wav', './hindi_data/givenFolderHSp/fullAudio/11janA.wav','./hindi_data/givenFolderPBData/fullAudio/8483_FullAudio.wav','./hindi_data/givenPMS/newGivenPMS/longAudio/april_v.wav' ]\n",
    "txts=['./hindi_data/givenFolderHSp/txt/12janA.txt', './hindi_data/givenFolderHSp/txt/11janA.txt','./hindi_data/givenFolderPBData/txt/8483_FullAudio.txt','./hindi_data/givenPMS/newGivenPMS/txt/april_v.txt']\n",
    "# files=['./hindi_data/givenFolderPBData/fullAudio/17162_FullAudio.wav']\n",
    "# txts=['./hindi_data/givenFolderPBData/txt/17162_FullAudio.txt']\n",
    "for txt_file_path, input_file in zip(txts, files):\n",
    "    %cd /home/suryansh/MADHAV\n",
    "    file=open(txt_file_path, 'r')\n",
    "    target=\"\"\n",
    "    target=file.readlines()\n",
    "    target=re.sub(\" +\", \" \", \" \".join(target))\n",
    "    # Now 'hindi_text' contains the contents of the file as a string\n",
    "#     print(target)\n",
    "    ground_truth.append(target)\n",
    "#     print(ground_truth)\n",
    "    model_dir = \"./FSMN-VAD\"\n",
    "    model = Fsmn_vad(model_dir, quantize=True)\n",
    "    result = model(input_file)\n",
    "    result = np.asarray(result, dtype=np.int32)\n",
    "    result=result.reshape((result.shape[1],2))\n",
    "    df=pd.DataFrame(result, columns=['start', 'end'])\n",
    "    # Split the audio into 15-second chunks with adjustment for the last chunk\n",
    "    chunks_array, split_sec = split_audio_using_VAD(input_file, df, 15)\n",
    "    %cd /home/suryansh/MADHAV/asr_train_asr_raw_hindi_bpe500\n",
    "\n",
    "    preds = \"\"\n",
    "    for i, chunk in enumerate(chunks_array):\n",
    "        start_time = time.time()\n",
    "        speech = np.array([])\n",
    "        duration=split_sec['end'][i]*1000-split_sec['start'][i] * 1000\n",
    "        for _, row in df.iterrows():\n",
    "            start_sample = row['start']-split_sec['start'][i] * 1000\n",
    "            end_sample = row['end']-split_sec['start'][i] * 1000\n",
    "    #         print(start_sample, end_sample, duration)\n",
    "            if(start_sample<0 and end_sample<0):\n",
    "                continue\n",
    "            if(start_sample>duration and end_sample>duration):\n",
    "                break\n",
    "    #         print(\"Y\")\n",
    "            speech = np.concatenate([speech, chunk[int(max(0,start_sample))*16:int(min(duration, end_sample))*16]])\n",
    "\n",
    "        print(len(speech))\n",
    "    #     nbests = speech2text(chunk)\n",
    "        if len(speech) != 0:\n",
    "            nbests = speech2text(speech)\n",
    "            text, *_ = nbests[0]\n",
    "    #         output_filename = f'results/output_chunk{i}.wav'\n",
    "    #         sf.write(output_filename, speech, samplerate=16000)\n",
    "            if(preds==\"\"):\n",
    "                preds=(text_normalizer(text))\n",
    "            else:\n",
    "                preds += \" \" + (text_normalizer(text))\n",
    "\n",
    "        print(i, \"/\", len(chunks_array))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "    predictions.append(preds)\n",
    "    torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d24f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a0e107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16e8e7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER: tensor(0.1550)\n",
      "WER: tensor(0.3401)\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text import CharErrorRate, WordErrorRate\n",
    "cer = CharErrorRate()\n",
    "print(\"CER:\", cer(predictions[:-1], ground_truth[:-1]))\n",
    "wer = WordErrorRate()\n",
    "print(\"WER:\", wer(predictions[:-1], ground_truth[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ba223fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER: tensor(0.1637)\n",
      "WER: tensor(0.3511)\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text import CharErrorRate, WordErrorRate\n",
    "cer = CharErrorRate()\n",
    "print(\"CER:\", cer(predictions[2], ground_truth[2]))\n",
    "wer = WordErrorRate()\n",
    "print(\"WER:\", wer(predictions[2], ground_truth[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef1fdd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER: tensor(0.7743)\n",
      "WER: tensor(0.9241)\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text import CharErrorRate, WordErrorRate\n",
    "cer = CharErrorRate()\n",
    "print(\"CER:\", cer(predictions[3], ground_truth[3]))\n",
    "wer = WordErrorRate()\n",
    "print(\"WER:\", wer(predictions[3], ground_truth[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "14aae280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/suryansh/MADHAV/asr_train_asr_raw_hindi_bpe500\n"
     ]
    }
   ],
   "source": [
    "%cd /home/suryansh/MADHAV/asr_train_asr_raw_hindi_bpe500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0680dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import string\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "from espnet2.bin.asr_inference import Speech2Text\n",
    "\n",
    "\n",
    "d = ModelDownloader()\n",
    "# It may takes a while to download and build models\n",
    "speech2text = Speech2Text(\n",
    "    \"exp/asr_train_asr_raw_hindi_bpe500/config.yaml\",\n",
    "    \"exp/asr_train_asr_raw_hindi_bpe500/valid.acc.ave_10best.pth\",\n",
    "    device=\"cuda\",\n",
    "    minlenratio=0.0,\n",
    "    maxlenratio=0.5,\n",
    "    ctc_weight=0.3,\n",
    "    beam_size=10,\n",
    "    batch_size=0,\n",
    "    nbest=1\n",
    ")\n",
    "\n",
    "def text_normalizer(text):\n",
    "    text = text.upper()\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "588e8ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suryansh/anaconda3/lib/python3.11/site-packages/espnet/nets/ctc_prefix_score.py:323: RuntimeWarning: invalid value encountered in logaddexp\n",
      "  r_sum = self.xp.logaddexp(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /500\n",
      "Time taken: 5.14 seconds\n",
      "1 /500\n",
      "Time taken: 5.37 seconds\n",
      "2 /500\n",
      "Time taken: 4.41 seconds\n",
      "3 /500\n",
      "Time taken: 2.48 seconds\n",
      "4 /500\n",
      "Time taken: 4.52 seconds\n",
      "5 /500\n",
      "Time taken: 4.49 seconds\n",
      "6 /500\n",
      "Time taken: 5.41 seconds\n",
      "7 /500\n",
      "Time taken: 5.35 seconds\n",
      "8 /500\n",
      "Time taken: 5.75 seconds\n",
      "9 /500\n",
      "Time taken: 5.21 seconds\n",
      "10 /500\n",
      "Time taken: 4.13 seconds\n",
      "11 /500\n",
      "Time taken: 5.15 seconds\n",
      "12 /500\n",
      "Time taken: 6.05 seconds\n",
      "13 /500\n",
      "Time taken: 5.67 seconds\n",
      "14 /500\n",
      "Time taken: 6.45 seconds\n",
      "15 /500\n",
      "Time taken: 5.18 seconds\n",
      "16 /500\n",
      "Time taken: 5.57 seconds\n",
      "17 /500\n",
      "Time taken: 5.97 seconds\n",
      "18 /500\n",
      "Time taken: 2.56 seconds\n",
      "19 /500\n",
      "Time taken: 5.08 seconds\n",
      "20 /500\n",
      "Time taken: 5.53 seconds\n",
      "21 /500\n",
      "Time taken: 2.00 seconds\n",
      "22 /500\n",
      "Time taken: 4.25 seconds\n",
      "23 /500\n",
      "Time taken: 5.51 seconds\n",
      "24 /500\n",
      "Time taken: 6.48 seconds\n",
      "25 /500\n",
      "Time taken: 5.60 seconds\n",
      "26 /500\n",
      "Time taken: 4.99 seconds\n",
      "27 /500\n",
      "Time taken: 5.11 seconds\n",
      "28 /500\n",
      "Time taken: 5.03 seconds\n",
      "29 /500\n",
      "Time taken: 5.10 seconds\n",
      "30 /500\n",
      "Time taken: 6.04 seconds\n",
      "31 /500\n",
      "Time taken: 5.09 seconds\n",
      "32 /500\n",
      "Time taken: 4.86 seconds\n",
      "33 /500\n",
      "Time taken: 3.95 seconds\n",
      "34 /500\n",
      "Time taken: 5.53 seconds\n",
      "35 /500\n",
      "Time taken: 5.50 seconds\n",
      "36 /500\n",
      "Time taken: 4.40 seconds\n",
      "37 /500\n",
      "Time taken: 3.22 seconds\n",
      "38 /500\n",
      "Time taken: 6.95 seconds\n",
      "39 /500\n",
      "Time taken: 4.72 seconds\n",
      "40 /500\n",
      "Time taken: 5.10 seconds\n",
      "41 /500\n",
      "Time taken: 5.30 seconds\n",
      "42 /500\n",
      "Time taken: 6.88 seconds\n",
      "43 /500\n",
      "Time taken: 2.71 seconds\n",
      "44 /500\n",
      "Time taken: 4.53 seconds\n",
      "45 /500\n",
      "Time taken: 5.81 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import soundfile\n",
    "import librosa.display\n",
    "from IPython.display import display, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "col_names=['path', 'text', 'time', 'sr', 'std']\n",
    "egs = pd.read_csv(\"/home/suryansh/MADHAV/hindi_data/givenFolderHSp/tsv/12janA.tsv\", sep=\"\\t\", names=col_names, header=None, encoding='utf-8', index_col=False)\n",
    "# print(egs['path'])\n",
    "preds=[]\n",
    "target=[]\n",
    "for index, row in egs.iterrows():\n",
    "    start_time = time.time()\n",
    "    speech, rate = soundfile.read(\"/home/suryansh/MADHAV/hindi_data/givenFolderHSp/finalSplitAudio/\" + row[\"path\"])\n",
    "#     assert fs == int(row[\"sr\"])\n",
    "    nbests = speech2text(speech)\n",
    "\n",
    "    text, *_ = nbests[0]\n",
    "# Uncomment to see details of each prediction\n",
    "#     print(f\"Input Speech: ESPNet_asr_egs/{row['path']}\")\n",
    "    # let us listen to samples\n",
    "#     display(Audio(speech, rate=rate))\n",
    "#     librosa.display.waveshow(speech, sr=rate, color=\"blue\")\n",
    "#     plt.show()\n",
    "#     print(f\"Reference text: {text_normalizer(row['text'])}\")\n",
    "    target.append(text_normalizer(row['text']))\n",
    "#     print(f\"ASR hypothesis: {text_normalizer(text)}\")\n",
    "    preds.append(text_normalizer(text))\n",
    "    print(index,\"/500\")\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "971af1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER: tensor(0.1677)\n",
      "WER: tensor(0.3445)\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text import CharErrorRate, WordErrorRate\n",
    "cer = CharErrorRate()\n",
    "print(\"CER:\", cer(preds, target))\n",
    "wer = WordErrorRate()\n",
    "print(\"WER:\", wer(preds, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b6010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
