{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6026e49",
   "metadata": {},
   "source": [
    "# Creating dataset and testing on Silero VAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b486a",
   "metadata": {},
   "source": [
    "#### Some cells are hidden and contained testing code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e8324e",
   "metadata": {},
   "source": [
    "<!--\n",
    "# import torch\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "# from IPython.display import Audio\n",
    "# from pprint import pprint\n",
    "# # download example\n",
    "# torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')\n",
    "\n",
    "# model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "#                               model='silero_vad',\n",
    "#                               force_reload=True)\n",
    "\n",
    "# (get_speech_timestamps,\n",
    "#  _, read_audio,\n",
    "#  *_) = utils\n",
    "\n",
    "# sampling_rate = 16000 # also accepts 8000\n",
    "# wav = read_audio('en_example.wav', sampling_rate=sampling_rate)\n",
    "# # get speech timestamps from full audio file\n",
    "# speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)\n",
    "# pprint(speech_timestamps)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757f05a",
   "metadata": {},
   "source": [
    "<!--\n",
    "# !ls\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062d0bd",
   "metadata": {},
   "source": [
    "<!--\n",
    "# import torchaudio\n",
    "# torchaudio.datasets.LIBRISPEECH('./librispeech', download=True)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6efd58",
   "metadata": {},
   "source": [
    "<!--\n",
    "# import os\n",
    "# import csv\n",
    "# import random\n",
    "# import pandas as pd\n",
    "# from pydub import AudioSegment\n",
    "# from pydub.generators import WhiteNoise\n",
    "\n",
    "# # Set the path to your base audio file\n",
    "# base_audio_path = \"./librispeech/70968/61-70968-0000.flac\"\n",
    "\n",
    "# # Set the output path for the combined audio\n",
    "# output_path = \"dataset\"\n",
    "# csv_file_path = os.path.join(output_path, \"regions_info.csv\")\n",
    "\n",
    "# # Load the base audio file\n",
    "# base_audio = AudioSegment.from_file(base_audio_path, 'flac')\n",
    "\n",
    "# # Function to insert random portions of noise between segments\n",
    "# def insert_noise_between_segments(base_audio, noise_duration=1000, num_segments=5):\n",
    "#     regions_info = []\n",
    "#     time=0\n",
    "#     combined_audio = AudioSegment.silent(duration=0)\n",
    "#     for _ in range(num_segments):\n",
    "#         # Randomly choose the start position for the noise segment\n",
    "#         start_position = random.randint(0, len(base_audio)//num_segments)\n",
    "#         # Extract the portion of the base audio before the insertion point\n",
    "#         combined_audio += base_audio[:start_position]\n",
    "#         regions_info.append((time, time+start_position))  # Mark speech region\n",
    "\n",
    "#         # Generate white noise\n",
    "#         noise = WhiteNoise().to_audio_segment(duration=noise_duration)\n",
    "#         noise = noise - noise.dBFS + base_audio.dBFS\n",
    "#         time+=start_position\n",
    "#         # Insert the noise segment\n",
    "#         combined_audio += noise\n",
    "#         time+=noise_duration\n",
    "#         # Update the base audio to start after the inserted noise segment\n",
    "#         base_audio = base_audio[start_position:]\n",
    "\n",
    "#     # Add any remaining portion of the base audio\n",
    "#     combined_audio += base_audio\n",
    "#     regions_info.append((time, len(combined_audio)))  # Mark speech region\n",
    "\n",
    "#     return combined_audio, regions_info\n",
    "\n",
    "# # Insert random portions of noise between segments of the base audio\n",
    "# combined_audio, regions_info = insert_noise_between_segments(base_audio, noise_duration=1000, num_segments=5)\n",
    "\n",
    "# # Save the combined audio\n",
    "# output_filename = \"combined_audio_with_noise.wav\"\n",
    "# output_file_path = os.path.join(output_path, output_filename)\n",
    "# combined_audio.export(output_file_path, format=\"wav\")\n",
    "\n",
    "# # Save the regions information to CSV using pandas DataFrame\n",
    "# df = pd.DataFrame(regions_info, columns=[\"start\", \"end\"])\n",
    "# df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# print(\"CSV file created successfully.\")\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47210ffe",
   "metadata": {},
   "source": [
    "<!--\n",
    "# sampling_rate = 16000 # also accepts 8000\n",
    "# wav = read_audio('./dataset/combined_audio_with_noise.wav', sampling_rate=sampling_rate)\n",
    "# # get speech timestamps from full audio file\n",
    "# speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)\n",
    "# pred = pd.DataFrame(speech_timestamps)\n",
    "# pred = pred // 16\n",
    "# print(pred)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef8f11",
   "metadata": {},
   "source": [
    "<!--\n",
    "# # Function to convert time stamps to millisecond-level intervals\n",
    "# def convert_to_milliseconds(dataframe):\n",
    "#     milliseconds_intervals = set()\n",
    "#     for _, row in dataframe.iterrows():\n",
    "#         for ms in range(row['start'], row['end'] + 1):\n",
    "#             milliseconds_intervals.add(ms)\n",
    "#     return milliseconds_intervals\n",
    "\n",
    "# number_set = set(range(1, df['start'].iloc[-1]))\n",
    "# # Convert ground truth and predictions to millisecond-level intervals\n",
    "# ground_truth_intervals = convert_to_milliseconds(df)\n",
    "# prediction_intervals_speech = convert_to_milliseconds(pred)\n",
    "\n",
    "# # Calculate true positives, false positives, and false negatives\n",
    "# true_positives_speech = len(ground_truth_intervals.intersection(prediction_intervals_speech))\n",
    "# false_positives_speech = len(prediction_intervals_speech - ground_truth_intervals)\n",
    "# false_negatives_speech = len(ground_truth_intervals - prediction_intervals_speech)\n",
    "\n",
    "# # Calculate precision and recall for speech class\n",
    "# precision_speech = true_positives_speech / (true_positives_speech + false_positives_speech) \\\n",
    "#     if true_positives_speech + false_positives_speech > 0 else 0\n",
    "\n",
    "# recall_speech = true_positives_speech / (true_positives_speech + false_negatives_speech) \\\n",
    "#     if true_positives_speech + false_negatives_speech > 0 else 0\n",
    "\n",
    "# # Print the results\n",
    "# print(f'Precision for speech class at millisecond level: {precision_speech:.2%}')\n",
    "# print(f'Recall for speech class at millisecond level: {recall_speech:.2%}')\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b32392",
   "metadata": {},
   "source": [
    "<!--\n",
    "# # Convert ground truth and predictions to millisecond-level intervals\n",
    "# ground_truth_intervals = number_set-convert_to_milliseconds(df)\n",
    "# prediction_intervals_speech = number_set-convert_to_milliseconds(pred)\n",
    "\n",
    "# # Calculate true positives, false positives, and false negatives\n",
    "# true_positives_speech = len(ground_truth_intervals.intersection(prediction_intervals_speech))\n",
    "# false_positives_speech = len(prediction_intervals_speech - ground_truth_intervals)\n",
    "# false_negatives_speech = len(ground_truth_intervals - prediction_intervals_speech)\n",
    "\n",
    "# # Calculate precision and recall for speech class\n",
    "# precision_speech = true_positives_speech / (true_positives_speech + false_positives_speech) \\\n",
    "#     if true_positives_speech + false_positives_speech > 0 else 0\n",
    "\n",
    "# recall_speech = true_positives_speech / (true_positives_speech + false_negatives_speech) \\\n",
    "#     if true_positives_speech + false_negatives_speech > 0 else 0\n",
    "\n",
    "# # Print the results\n",
    "# print(f'Precision for non-speech class at millisecond level: {precision_speech:.2%}')\n",
    "# print(f'Recall for non-speech class at millisecond level: {recall_speech:.2%}')\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc7f60eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "from pydub.generators import WhiteNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b8f3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "par=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "590f4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_noise_between_segments(output_file_path, base_audio, noise_duration=1000, num_segments=5):\n",
    "    regions_info = []\n",
    "    time=0\n",
    "    combined_audio = AudioSegment.silent(duration=0)\n",
    "    for _ in range(num_segments):\n",
    "        # Randomly choose the start position for the noise segment\n",
    "        start_position = random.randint(0, len(base_audio)//num_segments)\n",
    "        # Extract the portion of the base audio before the insertion point\n",
    "        combined_audio += base_audio[:start_position]\n",
    "        regions_info.append((output_file_path, time, time+start_position))  # Mark speech region\n",
    "\n",
    "        # Generate white noise\n",
    "        noise = WhiteNoise().to_audio_segment(duration=noise_duration)\n",
    "        noise = noise - noise.dBFS + base_audio.dBFS\n",
    "        time+=start_position\n",
    "        # Insert the noise segment\n",
    "        combined_audio += noise\n",
    "        time+=noise_duration\n",
    "        # Update the base audio to start after the inserted noise segment\n",
    "        base_audio = base_audio[start_position:]\n",
    "\n",
    "    # Add any remaining portion of the base audio\n",
    "    combined_audio += base_audio\n",
    "    regions_info.append((output_file_path, time, len(combined_audio)))  # Mark speech region\n",
    "\n",
    "    return combined_audio, regions_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f026b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert time stamps to millisecond-level intervals\n",
    "def convert_to_milliseconds(dataframe):\n",
    "    milliseconds_intervals = set()\n",
    "    for _, row in dataframe.iterrows():\n",
    "        for ms in range(row['start'], row['end'] + 1):\n",
    "            milliseconds_intervals.add(ms)\n",
    "    return milliseconds_intervals\n",
    "\n",
    "def evaluation_metrics(df, pred):\n",
    "    number_set = set(range(1, df['start'].iloc[-1]))\n",
    "    # Convert ground truth and predictions to millisecond-level intervals\n",
    "    ground_truth_intervals = convert_to_milliseconds(df)\n",
    "    prediction_intervals_speech = convert_to_milliseconds(pred)\n",
    "\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    true_positives_speech = len(ground_truth_intervals.intersection(prediction_intervals_speech))\n",
    "    false_positives_speech = len(prediction_intervals_speech - ground_truth_intervals)\n",
    "    false_negatives_speech = len(ground_truth_intervals - prediction_intervals_speech)\n",
    "\n",
    "    # Calculate precision and recall for speech class\n",
    "    precision_speech = true_positives_speech / (true_positives_speech + false_positives_speech) \\\n",
    "        if true_positives_speech + false_positives_speech > 0 else 0\n",
    "\n",
    "    recall_speech = true_positives_speech / (true_positives_speech + false_negatives_speech) \\\n",
    "        if true_positives_speech + false_negatives_speech > 0 else 0\n",
    "    # Convert ground truth and predictions to millisecond-level intervals\n",
    "    ground_truth_intervals = number_set-convert_to_milliseconds(df)\n",
    "    prediction_intervals_speech = number_set-convert_to_milliseconds(pred)\n",
    "\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    true_positives_speech = len(ground_truth_intervals.intersection(prediction_intervals_speech))\n",
    "    false_positives_speech = len(prediction_intervals_speech - ground_truth_intervals)\n",
    "    false_negatives_speech = len(ground_truth_intervals - prediction_intervals_speech)\n",
    "\n",
    "    # Calculate precision and recall for speech class\n",
    "    precision_non_speech = true_positives_speech / (true_positives_speech + false_positives_speech) \\\n",
    "        if true_positives_speech + false_positives_speech > 0 else 0\n",
    "\n",
    "    recall_non_speech = true_positives_speech / (true_positives_speech + false_negatives_speech) \\\n",
    "        if true_positives_speech + false_negatives_speech > 0 else 0\n",
    "    \n",
    "    return (precision_speech, recall_speech, precision_non_speech, recall_non_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50eeb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_names=['path', 'text', 'time', 'sr']\n",
    "# egs = pd.read_csv(\"../MADHAV_task_1/Task3/metadata_test_clean.tsv\", sep=\"\\t\", names=col_names, header=None)\n",
    "# egs=egs[:par]\n",
    "# preds=[]\n",
    "# target=[]\n",
    "# regions_info=[]\n",
    "# for index, row in egs.iterrows():\n",
    "#     base_audio_path = os.path.join(\"../MADHAV_task_1/Task3\", row[\"path\"].replace('originals/', ''))\n",
    "#     # Set the output path for the combined audio\n",
    "#     output_path = \"dataset\"\n",
    "#     csv_file_path = os.path.join(output_path, \"regions_info.csv\")\n",
    "\n",
    "#     # Load the base audio file\n",
    "#     base_audio = AudioSegment.from_file(base_audio_path, 'flac')\n",
    "#     output_filename = f\"audio_noise{index}.wav\"\n",
    "#     output_file_path = os.path.join(output_path, output_filename)\n",
    "#     # Insert random portions of noise between segments of the base audio\n",
    "#     combined_audio, info = insert_noise_between_segments(output_file_path, base_audio, noise_duration=1000, num_segments=5)\n",
    "#     regions_info=regions_info+info\n",
    "#     # Save the combined audio\n",
    "#     combined_audio.export(output_file_path, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e85888fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the regions information to CSV using pandas DataFrame\n",
    "# df = pd.DataFrame(regions_info, columns=[\"path\",\"start\", \"end\"])\n",
    "# df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# print(\"CSV file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2540e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"dataset/regions_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad6ac26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1.83M/1.83M [00:07<00:00, 243kB/s]\n",
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to /home/suryansh/.cache/torch/hub/master.zip\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "from IPython.display import Audio\n",
    "from pprint import pprint\n",
    "# download example\n",
    "torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', 'en_example.wav')\n",
    "\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " _, read_audio,\n",
    " *_) = utils\n",
    "sampling_rate=16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a5e49a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [04:21<00:00,  3.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "evaluation=[]\n",
    "for i in tqdm(range(par)):\n",
    "    path=f\"dataset/audio_noise{i}.wav\"\n",
    "    \n",
    "    df1=df[df['path']==path]\n",
    "    wav = read_audio(path, sampling_rate=sampling_rate)\n",
    "    # get speech timestamps from full audio file\n",
    "    speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)\n",
    "    pred = pd.DataFrame(speech_timestamps)\n",
    "    pred = pred // 16\n",
    "    evaluation.append(evaluation_metrics(df1, pred))\n",
    "eval_df=pd.DataFrame(evaluation, columns=['precision_speech', 'recall_speech', 'precision_non_speech', 'recall_non_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3941b109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision_speech        0.772603\n",
       "recall_speech           0.796883\n",
       "precision_non_speech    0.856014\n",
       "recall_non_speech       0.731508\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f1f0646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1000/1000 [04:01<00:00,  4.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "evaluation=[]\n",
    "for i in tqdm(range(par)):\n",
    "    path=f\"mixed_audio/mixed_audio_noise{i}.wav\"\n",
    "    \n",
    "    df1=df[df['path']==f\"dataset/audio_noise{i}.wav\"]\n",
    "    wav = read_audio(path, sampling_rate=sampling_rate)\n",
    "    # get speech timestamps from full audio file\n",
    "    speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=sampling_rate)\n",
    "    pred = pd.DataFrame(speech_timestamps)\n",
    "    pred = pred // 16\n",
    "    evaluation.append(evaluation_metrics(df1, pred))\n",
    "eval_df=pd.DataFrame(evaluation, columns=['precision_speech', 'recall_speech', 'precision_non_speech', 'recall_non_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9899e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision_speech        0.812780\n",
       "recall_speech           0.814681\n",
       "precision_non_speech    0.860896\n",
       "recall_non_speech       0.793615\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321fb63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
