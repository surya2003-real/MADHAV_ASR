{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03e69edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" VAD pipeline, modified from <https://github.com/wiseman/py-webrtcvad/blob/master/example.py> \"\"\"\n",
    "import os\n",
    "import collections\n",
    "import contextlib\n",
    "import sys\n",
    "import wave\n",
    "import pandas as pd\n",
    "import webrtcvad\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "762de0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "par=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed6e6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert time stamps to millisecond-level intervals\n",
    "def convert_to_milliseconds(dataframe):\n",
    "    milliseconds_intervals = set()\n",
    "    for _, row in dataframe.iterrows():\n",
    "        for ms in range(int(row['start']), int(row['end']) + 1):\n",
    "            milliseconds_intervals.add(ms)\n",
    "    return milliseconds_intervals\n",
    "\n",
    "def evaluation_metrics(df, pred):\n",
    "    number_set = set(range(1, df['end'].iloc[-1]))\n",
    "    # Convert ground truth and predictions to millisecond-level intervals\n",
    "    ground_truth_intervals = convert_to_milliseconds(df)\n",
    "    prediction_intervals_speech = convert_to_milliseconds(pred)\n",
    "\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    true_positives_speech = len(ground_truth_intervals.intersection(prediction_intervals_speech))\n",
    "    false_positives_speech = len(prediction_intervals_speech - ground_truth_intervals)\n",
    "    false_negatives_speech = len(ground_truth_intervals - prediction_intervals_speech)\n",
    "\n",
    "    # Calculate precision and recall for speech class\n",
    "    precision_speech = true_positives_speech / (true_positives_speech + false_positives_speech) \\\n",
    "        if true_positives_speech + false_positives_speech > 0 else 0\n",
    "\n",
    "    recall_speech = true_positives_speech / (true_positives_speech + false_negatives_speech) \\\n",
    "        if true_positives_speech + false_negatives_speech > 0 else 0\n",
    "    # Convert ground truth and predictions to millisecond-level intervals\n",
    "    ground_truth_intervals = number_set-convert_to_milliseconds(df)\n",
    "    prediction_intervals_speech = number_set-convert_to_milliseconds(pred)\n",
    "\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    true_positives_speech = len(ground_truth_intervals.intersection(prediction_intervals_speech))\n",
    "    false_positives_speech = len(prediction_intervals_speech - ground_truth_intervals)\n",
    "    false_negatives_speech = len(ground_truth_intervals - prediction_intervals_speech)\n",
    "\n",
    "    # Calculate precision and recall for speech class\n",
    "    precision_non_speech = true_positives_speech / (true_positives_speech + false_positives_speech) \\\n",
    "        if true_positives_speech + false_positives_speech > 0 else 0\n",
    "\n",
    "    recall_non_speech = true_positives_speech / (true_positives_speech + false_negatives_speech) \\\n",
    "        if true_positives_speech + false_negatives_speech > 0 else 0\n",
    "    \n",
    "    return (precision_speech, recall_speech, precision_non_speech, recall_non_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "960204bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wave(path):\n",
    "    \"\"\"Reads a .wav file.\n",
    "    Takes the path, and returns (PCM audio data, sample rate).\n",
    "    \"\"\"\n",
    "    with contextlib.closing(wave.open(path, 'rb')) as wavfile:\n",
    "        num_channels = wavfile.getnchannels()\n",
    "        assert num_channels == 1\n",
    "        sample_width = wavfile.getsampwidth()\n",
    "        assert sample_width == 2\n",
    "        sample_rate = wavfile.getframerate()\n",
    "#         assert sample_rate in (8000, 16000, 32000, 48000)\n",
    "        pcm_data = wavfile.readframes(wavfile.getnframes())\n",
    "        return pcm_data, sample_rate\n",
    "\n",
    "def write_wave(path, audio, sample_rate):\n",
    "    \"\"\"Writes a .wav file.\n",
    "    Takes path, PCM audio data, and sample rate.\n",
    "    \"\"\"\n",
    "    with contextlib.closing(wave.open(path, 'wb')) as wavfile:\n",
    "        wavfile.setnchannels(1)\n",
    "        wavfile.setsampwidth(2)\n",
    "        wavfile.setframerate(sample_rate)\n",
    "        wavfile.writeframes(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d457ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frame(object):\n",
    "    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n",
    "    def __init__(self, bytes, timestamp, duration):\n",
    "        self.bytes = bytes\n",
    "        self.timestamp = timestamp\n",
    "        self.duration = duration\n",
    "\n",
    "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
    "    \"\"\"Generates audio frames from PCM audio data.\n",
    "    Takes the desired frame duration in milliseconds, the PCM data, and\n",
    "    the sample rate.\n",
    "    Yields Frames of the requested duration.\n",
    "    \"\"\"\n",
    "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
    "    offset = 0\n",
    "    timestamp = 0.0\n",
    "    duration = (float(n) / sample_rate) / 2.0\n",
    "    while offset + n < len(audio):\n",
    "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
    "        timestamp += duration\n",
    "        offset += n\n",
    "\n",
    "# Changed output to frames instead of binary data\n",
    "def vad_collector(sample_rate, frame_duration_ms,\n",
    "                  padding_duration_ms, vad, frames):\n",
    "    \"\"\"Filters out non-voiced audio frames.\n",
    "    Given a webrtcvad.Vad and a source of audio frames, yields only\n",
    "    the voiced audio.\n",
    "    Uses a padded, sliding window algorithm over the audio frames.\n",
    "    When more than 90% of the frames in the window are voiced (as\n",
    "    reported by the VAD), the collector triggers and begins yielding\n",
    "    audio frames. Then the collector waits until 90% of the frames in\n",
    "    the window are unvoiced to detrigger.\n",
    "    The window is padded at the front and back to provide a small\n",
    "    amount of silence or the beginnings/endings of speech around the\n",
    "    voiced frames.\n",
    "    Arguments:\n",
    "    sample_rate - The audio sample rate, in Hz.\n",
    "    frame_duration_ms - The frame duration in milliseconds.\n",
    "    padding_duration_ms - The amount to pad the window, in milliseconds.\n",
    "    vad - An instance of webrtcvad.Vad.\n",
    "    frames - a source of audio frames (sequence or generator).\n",
    "    Returns: A generator that yields PCM audio data.\n",
    "    \"\"\"\n",
    "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
    "    # We use a deque for our sliding window/ring buffer.\n",
    "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
    "    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n",
    "    # NOTTRIGGERED state.\n",
    "    triggered = False\n",
    "\n",
    "    voiced_frames = []\n",
    "    w=0\n",
    "    for frame in frames:\n",
    "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
    "\n",
    "        # sys.stdout.write('1' if is_speech else '0')\n",
    "        if not triggered:\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
    "            # If we're NOTTRIGGERED and more than 90% of the frames in\n",
    "            # the ring buffer are voiced frames, then enter the\n",
    "            # TRIGGERED state.\n",
    "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
    "                triggered = True\n",
    "                # sys.stdout.write('+(%s)' % (ring_buffer[0][0].timestamp,))\n",
    "\n",
    "                # We want to yield all the audio we see from now until\n",
    "                # we are NOTTRIGGERED, but we have to start with the\n",
    "                # audio that's already in the ring buffer.\n",
    "                for tmp_frame, tmp_speech in ring_buffer:\n",
    "                    voiced_frames.append(tmp_frame)\n",
    "                ring_buffer.clear()\n",
    "        else:\n",
    "            # We're in the TRIGGERED state, so collect the audio data\n",
    "            # and add it to the ring buffer.\n",
    "            voiced_frames.append(frame)\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
    "            # If more than 90% of the frames in the ring buffer are\n",
    "            # unvoiced, then enter NOTTRIGGERED and yield whatever\n",
    "            # audio we've collected.\n",
    "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
    "                # sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n",
    "                triggered = False\n",
    "                # yield b''.join([f.bytes for f in voiced_frames])\n",
    "                yield voiced_frames\n",
    "                ring_buffer.clear()\n",
    "                voiced_frames = []\n",
    "    if triggered:\n",
    "        # sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n",
    "        pass\n",
    "\n",
    "    # sys.stdout.write('\\n')\n",
    "    # If we have any leftover voiced audio when we run out of input,\n",
    "    # yield it.\n",
    "\n",
    "    if voiced_frames:\n",
    "        # yield b''.join([f.bytes for f in voiced_frames])\n",
    "        yield voiced_frames\n",
    "\n",
    "def main(args):\n",
    "    if len(args) != 2:\n",
    "        sys.stderr.write(\n",
    "            'Usage: example.py <aggressiveness> <path to wav file>\\n')\n",
    "        sys.exit(1)\n",
    "    audio, sample_rate = read_wave(args[1])\n",
    "    vad = webrtcvad.Vad(int(args[0]))\n",
    "    frames = frame_generator(30, audio, sample_rate)\n",
    "    frames = list(frames)\n",
    "    segments = vad_collector(sample_rate, 30, 300, vad, frames)\n",
    "    for i, segment in enumerate(segments):\n",
    "        path = 'chunk-%002d.wav' % (i,)\n",
    "        print(' Writing %s' % (path,))\n",
    "        write_wave(path, segment, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ce207f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2segments(wavfile, mode=3, outputdir=None, max_duration_ms=20000):\n",
    "    \"\"\" Convert an wavfile to voiced segments\n",
    "\n",
    "    Returns:\n",
    "        list_timestamp: list of dict, {\"id\": i, \"start\": start, \"stop\": stop}\n",
    "        list_wavpath: list of strings\n",
    "    \"\"\"\n",
    "    frame_duration_ms = 30\n",
    "    padding_duration_ms = 300\n",
    "    audio, sample_rate = read_wave(wavfile)\n",
    "    vad = webrtcvad.Vad(mode)\n",
    "    frames = frame_generator(frame_duration_ms, audio, sample_rate)\n",
    "    frames = list(frames)\n",
    "    segments = vad_collector(sample_rate, frame_duration_ms, padding_duration_ms, vad, frames)\n",
    "    max_number_frame = int(max_duration_ms/frame_duration_ms)\n",
    "    segments = split_too_long(segments, max_number_frame)\n",
    "\n",
    "    list_timestamps = []\n",
    "    list_wavpath = []\n",
    "    for i, voiced_frames in enumerate(segments):\n",
    "        start = voiced_frames[0].timestamp\n",
    "        stop = voiced_frames[-1].timestamp + voiced_frames[-1].duration\n",
    "        timestamp = {\"id\": i, \"start\": round(start, 4), \"stop\": round(stop, 4)}\n",
    "        # timestamp = {\"id\": i, \"start\": segment.timestamp, \"stop\": segment.duration}\n",
    "        list_timestamps.append(timestamp)\n",
    "\n",
    "        if outputdir is not None:\n",
    "            if os.path.isdir(outputdir):\n",
    "                segment = b''.join([f.bytes for f in voiced_frames])\n",
    "                segment_path = os.path.join(outputdir, \"chunk-{}.wav\".format(str(i).zfill(4)))\n",
    "                list_wavpath.append(segment_path)\n",
    "                write_wave(segment_path, segment, sample_rate)\n",
    "    return list_timestamps, list_wavpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f4a8e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_too_long(segments, max_number_frame):\n",
    "    \"\"\" Limit the number of frames of each segment.\n",
    "    Args:\n",
    "        segments: list of segment, and each segment is a list of frame\n",
    "        max_number_frame: int\n",
    "    Returns:\n",
    "        new_segments: list of segment\n",
    "    \"\"\"\n",
    "    new_segments = []\n",
    "    for voiced_frames in segments:\n",
    "        new_voiced_frames = []\n",
    "        counter = 0\n",
    "        for frame in voiced_frames:\n",
    "            if counter == max_number_frame:\n",
    "                new_segments.append(new_voiced_frames)\n",
    "                new_voiced_frames = []\n",
    "                counter = 0\n",
    "            new_voiced_frames.append(frame)\n",
    "            counter+=1\n",
    "        if counter > 0:\n",
    "            new_segments.append(new_voiced_frames)\n",
    "    return new_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "824dc6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.51</td>\n",
       "      <td>13.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.89</td>\n",
       "      <td>14.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.94</td>\n",
       "      <td>18.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start    end\n",
       "0   0.51  13.53\n",
       "1  13.89  14.91\n",
       "2  14.94  18.84"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wavpath = './output/audio_noise1.wav'\n",
    "dst_out = './dest'\n",
    "timestamps, _ = wav2segments(wavpath, mode=3, outputdir=dst_out)\n",
    "df=pd.DataFrame(timestamps)\n",
    "df['end']=df['stop']\n",
    "df.drop(['stop','id'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "655eb266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1000/1000 [00:03<00:00, 260.07it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluation=[]\n",
    "df=pd.read_csv(\"dataset/regions_info.csv\")\n",
    "for i in tqdm(range(par)):\n",
    "    audio_path = f\"output/audio_noise{i}.wav\"\n",
    "    df1=df[df['path']==f\"dataset/audio_noise{i}.wav\"]\n",
    "    #get predictions\n",
    "    timestamps, _ = wav2segments(audio_path, mode=1)\n",
    "    #evaluate\n",
    "    pred=pd.DataFrame(timestamps)\n",
    "    pred['end']=pred['stop']\n",
    "    pred.drop(['stop', 'id'], axis=1, inplace=True)\n",
    "    pred=pred*1000\n",
    "    evaluation.append(evaluation_metrics(df1, pred))\n",
    "eval_df=pd.DataFrame(evaluation, columns=['precision_speech', 'recall_speech', 'precision_non_speech', 'recall_non_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "be90088f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision_speech        0.524036\n",
       "recall_speech           0.945095\n",
       "precision_non_speech    0.013005\n",
       "recall_non_speech       0.003324\n",
       "dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "842c6d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1000/1000 [00:03<00:00, 272.27it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "evaluation=[]\n",
    "df=pd.read_csv(\"dataset/regions_info.csv\")\n",
    "for i in tqdm(range(par)):\n",
    "    audio_path = f\"mixed_audio/mixed_audio_noise{i}.wav\"\n",
    "    df1=df[df['path']==f\"dataset/audio_noise{i}.wav\"]\n",
    "    #get predictions\n",
    "    timestamps, _ = wav2segments(audio_path, mode=1)\n",
    "    #evaluate\n",
    "    pred=pd.DataFrame(timestamps)\n",
    "    pred['end']=pred['stop']\n",
    "    pred.drop(['stop', 'id'], axis=1, inplace=True)\n",
    "    pred=pred*1000\n",
    "    evaluation.append(evaluation_metrics(df1, pred))\n",
    "eval_df=pd.DataFrame(evaluation, columns=['precision_speech', 'recall_speech', 'precision_non_speech', 'recall_non_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a6eec39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision_speech        0.600089\n",
       "recall_speech           0.863516\n",
       "precision_non_speech    0.297119\n",
       "recall_non_speech       0.248599\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0df53e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
