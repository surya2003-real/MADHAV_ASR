{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e920d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c619ed",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb87cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a0ddac8",
   "metadata": {},
   "source": [
    "## Model algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b6810f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_using_VAD(input_file, pred, chunk_length_sec=15):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(input_file, sr=None)\n",
    "    timestamps_df=pred*sr/1000\n",
    "    # Calculate the total duration of the audio in samples\n",
    "    total_samples = len(audio)\n",
    "\n",
    "    # Calculate the chunk length in samples\n",
    "    chunk_length_samples = int(sr * chunk_length_sec)\n",
    "\n",
    "    # Create an array to store audio chunks\n",
    "    audio_chunks = []\n",
    "    split_sec=[]\n",
    "    k=0\n",
    "    index=0\n",
    "    while k+chunk_length_samples<total_samples:\n",
    "        start_sample = k\n",
    "        end_sample = k+chunk_length_samples\n",
    "        nearest_timestamp_index = np.abs(np.array(timestamps_df['end']>start_sample) - end_sample).argmin()\n",
    "        nearest_timestamp_end_sample = int(timestamps_df.loc[nearest_timestamp_index, 'end'])\n",
    "        end_sample = min(end_sample, nearest_timestamp_end_sample)\n",
    "        chunk = audio[start_sample:end_sample]\n",
    "        audio_chunks.append(chunk)\n",
    "        split_sec.append((index, start_sample/sr, end_sample/sr))\n",
    "        k=end_sample\n",
    "        index+=1\n",
    "\n",
    "    last_chunk = audio[k:total_samples]\n",
    "    audio_chunks.append(last_chunk)\n",
    "    split_sec.append((index, k/sr, total_samples/sr))\n",
    "    split_sec=pd.DataFrame(split_sec, columns=['index', 'start', 'end'])\n",
    "    return audio_chunks, split_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6c1e8d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110</td>\n",
       "      <td>2640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2920</td>\n",
       "      <td>4360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4690</td>\n",
       "      <td>5290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5810</td>\n",
       "      <td>6900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7990</td>\n",
       "      <td>10270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10690</td>\n",
       "      <td>18890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start    end\n",
       "0    110   2640\n",
       "1   2920   4360\n",
       "2   4690   5290\n",
       "3   5810   6900\n",
       "4   7990  10270\n",
       "5  10690  18890"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fsmnvad import FSMNVad\n",
    "# Specify the input audio file\n",
    "input_file = \"output/audio_noise1.wav\"\n",
    "vad = FSMNVad()\n",
    "segments = vad.segments_offline(input_file)\n",
    "df=pd.DataFrame(segments, columns=['start', 'end'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "93727768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: start: 0.0 end: 2.64\n",
      "Chunk 1: start: 2.64 end: 4.36\n",
      "Chunk 2: start: 4.36 end: 5.29\n",
      "Chunk 3: start: 5.29 end: 6.9\n",
      "Chunk 4: start: 6.9 end: 10.27\n",
      "Chunk 5: start: 10.27 end: 15.27\n",
      "Chunk 6: start: 15.27 end: 18.90975\n"
     ]
    }
   ],
   "source": [
    "# Split the audio into 15-second chunks with adjustment for the last chunk\n",
    "chunks_array, split_sec = split_audio_using_VAD(input_file, df, 5)\n",
    "\n",
    "# Now, chunks_array contains the audio chunks as numpy arrays\n",
    "for i, chunk in enumerate(chunks_array):\n",
    "    print(f\"Chunk {i}: start: {split_sec['start'][i]} end: {split_sec['end'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e5d01511",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q espnet_model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1f309580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Choose English ASR model { run: \"auto\" }\n",
    "\n",
    "lang = 'en'\n",
    "fs = 16000 #@param {type:\"integer\"}\n",
    "tag = 'Shinji Watanabe/spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid.acc.ave' #@param [\"Shinji Watanabe/spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid.acc.ave\", \"kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave\"] {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4a7a8ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,611 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,611 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,611 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,611 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,611 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,611 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,611 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,611 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,611 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,611 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,611 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "WARNING:root:Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,652 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,652 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,652 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,652 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,652 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,652 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,652 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,652 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,652 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,652 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 16:57:39,652 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import string\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "from espnet2.bin.asr_inference import Speech2Text\n",
    "\n",
    "\n",
    "d = ModelDownloader()\n",
    "# It may takes a while to download and build models\n",
    "speech2text = Speech2Text(\n",
    "    **d.download_and_unpack(tag),\n",
    "    device=\"cpu\",\n",
    "    minlenratio=0.0,\n",
    "    maxlenratio=0.0,\n",
    "    ctc_weight=0.3,\n",
    "    beam_size=10,\n",
    "    batch_size=0,\n",
    "    nbest=1\n",
    ")\n",
    "\n",
    "def text_normalizer(text):\n",
    "    text = text.upper()\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3eb85108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40480\n",
      "0 /500\n",
      "Time taken: 7.03 seconds\n",
      "23040\n",
      "1 /500\n",
      "Time taken: 8.91 seconds\n",
      "9600\n",
      "2 /500\n",
      "Time taken: 3.00 seconds\n",
      "17440\n",
      "3 /500\n",
      "Time taken: 5.99 seconds\n",
      "36480\n",
      "4 /500\n",
      "Time taken: 6.60 seconds\n",
      "73280\n",
      "5 /500\n",
      "Time taken: 11.02 seconds\n",
      "57920\n",
      "6 /500\n",
      "Time taken: 14.16 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import librosa.display\n",
    "from IPython.display import display, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "col_names = ['path', 'text', 'time', 'sr']\n",
    "preds = \"\"\n",
    "target = \"\"\n",
    "\n",
    "for i, chunk in enumerate(chunks_array):\n",
    "    start_time = time.time()\n",
    "    speech = np.array([])\n",
    "    duration=split_sec['end'][i]*1000-split_sec['start'][i] * 1000\n",
    "    for _, row in df.iterrows():\n",
    "        start_sample = row['start']-split_sec['start'][i] * 1000\n",
    "        end_sample = row['end']-split_sec['start'][i] * 1000\n",
    "#         print(start_sample, end_sample, duration)\n",
    "        if(start_sample<0 and end_sample<0):\n",
    "            continue\n",
    "        if(start_sample>duration and end_sample>duration):\n",
    "            continue\n",
    "#         print(\"Y\")\n",
    "        speech = np.concatenate([speech, chunk[int(max(0,start_sample))*16:int(min(duration, end_sample))*16]])\n",
    "\n",
    "    print(len(speech))\n",
    "    \n",
    "    if len(speech) != 0:\n",
    "        nbests = speech2text(speech)\n",
    "        text, *_ = nbests[0]\n",
    "#         output_filename = f'results/output_chunk{i}.wav'\n",
    "#         sf.write(output_filename, speech, samplerate=16000)\n",
    "        if(preds==\"\"):\n",
    "            preds=(text_normalizer(text))\n",
    "        else:\n",
    "            preds += \" \" + (text_normalizer(text))\n",
    "\n",
    "    print(i, \"/500\")\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "956a1f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHE TRIED THIS MORNING AND ERIC WERE GOING TO  ILL PASS IT TO YOU SO THATS THE FROM THE PNL AND STAYING A SIMPLE SONG AND ITS SWEET BUT ITS SLIGHTLY METALLIC VOIDS AND THEN SEEDING YOURSELVES BY THE OPEN WINDOW RED PHILIPS PLATTER\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c0788f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER: tensor(1.1503)\n",
      "WER: tensor(1.3333)\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text import CharErrorRate, WordErrorRate\n",
    "target=\"she tried this morning an air or two upon the piano sang a simple song in a sweet but slightly metallic voice and then seating herself by the open window read philips letter\"\n",
    "cer = CharErrorRate()\n",
    "print(\"CER:\", cer(preds, target))\n",
    "wer = WordErrorRate()\n",
    "print(\"WER:\", wer(preds, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424f119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
