{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8468023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f4323",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d25bf0",
   "metadata": {},
   "source": [
    "## Model algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e33ac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio_using_VAD(input_file, pred, chunk_length_sec=15):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(input_file, sr=None)\n",
    "    timestamps_df=pred*sr/1000\n",
    "    # Calculate the total duration of the audio in samples\n",
    "    total_samples = len(audio)\n",
    "\n",
    "    # Calculate the chunk length in samples\n",
    "    chunk_length_samples = int(sr * chunk_length_sec)\n",
    "\n",
    "    # Create an array to store audio chunks\n",
    "    audio_chunks = []\n",
    "    split_sec=[]\n",
    "    k=0\n",
    "    index=0\n",
    "    while k+chunk_length_samples<total_samples:\n",
    "        start_sample = k\n",
    "        end_sample = k+chunk_length_samples\n",
    "        nearest_timestamp_index = np.abs(np.array(timestamps_df['end']>start_sample) - end_sample).argmin()\n",
    "        nearest_timestamp_end_sample = int(timestamps_df.loc[nearest_timestamp_index, 'end'])\n",
    "        end_sample = min(end_sample, nearest_timestamp_end_sample)\n",
    "        chunk = audio[start_sample:end_sample]\n",
    "        audio_chunks.append(chunk)\n",
    "        split_sec.append((index, start_sample/sr, end_sample/sr))\n",
    "        k=end_sample\n",
    "        index+=1\n",
    "\n",
    "    last_chunk = audio[k:total_samples]\n",
    "    audio_chunks.append(last_chunk)\n",
    "    split_sec.append((index, k/sr, total_samples/sr))\n",
    "    split_sec=pd.DataFrame(split_sec, columns=['index', 'start', 'end'])\n",
    "    return audio_chunks, split_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "940da8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110</td>\n",
       "      <td>2640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2920</td>\n",
       "      <td>4360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4690</td>\n",
       "      <td>5290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5810</td>\n",
       "      <td>6900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7990</td>\n",
       "      <td>10270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10690</td>\n",
       "      <td>18890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start    end\n",
       "0    110   2640\n",
       "1   2920   4360\n",
       "2   4690   5290\n",
       "3   5810   6900\n",
       "4   7990  10270\n",
       "5  10690  18890"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fsmnvad import FSMNVad\n",
    "# Specify the input audio file\n",
    "input_file = \"output/audio_noise1.wav\"\n",
    "vad = FSMNVad()\n",
    "segments = vad.segments_offline(input_file)\n",
    "df=pd.DataFrame(segments, columns=['start', 'end'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bde2bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: start: 0.0 end: 2.64\n",
      "Chunk 1: start: 2.64 end: 4.36\n",
      "Chunk 2: start: 4.36 end: 5.29\n",
      "Chunk 3: start: 5.29 end: 6.9\n",
      "Chunk 4: start: 6.9 end: 10.27\n",
      "Chunk 5: start: 10.27 end: 18.27\n",
      "Chunk 6: start: 18.27 end: 18.90975\n"
     ]
    }
   ],
   "source": [
    "# Split the audio into 15-second chunks with adjustment for the last chunk\n",
    "chunks_array, split_sec = split_audio_using_VAD(input_file, df, 8)\n",
    "\n",
    "# Now, chunks_array contains the audio chunks as numpy arrays\n",
    "for i, chunk in enumerate(chunks_array):\n",
    "    print(f\"Chunk {i}: start: {split_sec['start'][i]} end: {split_sec['end'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d710c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q espnet_model_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c40b1949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Choose English ASR model { run: \"auto\" }\n",
    "\n",
    "lang = 'en'\n",
    "fs = 16000 #@param {type:\"integer\"}\n",
    "tag = 'Shinji Watanabe/spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid.acc.ave' #@param [\"Shinji Watanabe/spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid.acc.ave\", \"kamo-naoyuki/librispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe5000_scheduler_confwarmup_steps40000_optim_conflr0.0025_sp_valid.acc.ave\"] {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e239e381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,527 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,527 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,527 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,527 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,527 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,527 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,527 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,527 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,527 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,527 WARNING [conformer_encoder.py:139] Using legacy_rel_pos and it will be deprecated in the future.\n",
      "WARNING:root:Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,556 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,556 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,556 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,556 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,556 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,556 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,556 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,556 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,556 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n",
      "2023-12-11 14:47:50,556 WARNING [conformer_encoder.py:246] Using legacy_rel_selfattn and it will be deprecated in the future.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import string\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "from espnet2.bin.asr_inference import Speech2Text\n",
    "\n",
    "\n",
    "d = ModelDownloader()\n",
    "# It may takes a while to download and build models\n",
    "speech2text = Speech2Text(\n",
    "    **d.download_and_unpack(tag),\n",
    "    device=\"cpu\",\n",
    "    minlenratio=0.0,\n",
    "    maxlenratio=0.0,\n",
    "    ctc_weight=0.3,\n",
    "    beam_size=10,\n",
    "    batch_size=0,\n",
    "    nbest=1\n",
    ")\n",
    "\n",
    "def text_normalizer(text):\n",
    "    text = text.upper()\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "36b2ec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   start     end\n",
      "0  110.0  2640.0\n",
      "(40480,)\n",
      "40480\n",
      "0 /500\n",
      "Time taken: 9.33 seconds\n",
      "    start     end\n",
      "0 -2530.0     0.0\n",
      "1   280.0  1720.0\n",
      "(0,)\n",
      "(23040,)\n",
      "23040\n",
      "1 /500\n",
      "Time taken: 12.18 seconds\n",
      "    start    end\n",
      "1 -1440.0    0.0\n",
      "2   330.0  930.0\n",
      "(0,)\n",
      "(9600,)\n",
      "9600\n",
      "2 /500\n",
      "Time taken: 4.65 seconds\n",
      "   start     end\n",
      "2 -600.0     0.0\n",
      "3  520.0  1610.0\n",
      "(0,)\n",
      "(17440,)\n",
      "17440\n",
      "3 /500\n",
      "Time taken: 8.87 seconds\n",
      "    start     end\n",
      "3 -1090.0     0.0\n",
      "4  1090.0  3370.0\n",
      "(0,)\n",
      "(36480,)\n",
      "36480\n",
      "4 /500\n",
      "Time taken: 9.97 seconds\n",
      "    start  end\n",
      "4 -2280.0  0.0\n",
      "(0,)\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2D or 3D (batch mode) tensor with possibly 0 batch size and other non-zero dimensions for input, but got: [1, 1, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(speech))\n\u001b[0;32m---> 26\u001b[0m     nbests \u001b[38;5;241m=\u001b[39m speech2text(speech)\n\u001b[1;32m     27\u001b[0m     text, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m nbests[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Uncomment to see details of each prediction\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     print(f\"Input Speech: ESPNet_asr_egs/{row['path']}\")\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# let us listen to samples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     print(f\"Reference text: {text_normalizer(row['text'])}\")\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#     print(f\"ASR hypothesis: {text_normalizer(text)}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet2/bin/asr_inference.py:498\u001b[0m, in \u001b[0;36mSpeech2Text.__call__\u001b[0;34m(self, speech)\u001b[0m\n\u001b[1;32m    495\u001b[0m batch \u001b[38;5;241m=\u001b[39m to_device(batch, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# b. Forward Encoder\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m enc, enc_olens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masr_model\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_asr:\n\u001b[1;32m    500\u001b[0m     enc \u001b[38;5;241m=\u001b[39m enc\u001b[38;5;241m.\u001b[39munbind(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch, num_inf, ...) -> num_inf x [batch, ...]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet2/asr/espnet_model.py:378\u001b[0m, in \u001b[0;36mESPnetASRModel.encode\u001b[0;34m(self, speech, speech_lengths)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Frontend + Encoder. Note that this method is used by asr_inference.py\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    speech: (Batch, Length, ...)\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    speech_lengths: (Batch, )\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# 1. Extract feats\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     feats, feats_lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_feats(speech, speech_lengths)\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# 2. Data augmentation\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecaug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet2/asr/espnet_model.py:443\u001b[0m, in \u001b[0;36mESPnetASRModel._extract_feats\u001b[0;34m(self, speech, speech_lengths)\u001b[0m\n\u001b[1;32m    436\u001b[0m speech \u001b[38;5;241m=\u001b[39m speech[:, : speech_lengths\u001b[38;5;241m.\u001b[39mmax()]\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrontend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;66;03m# Frontend\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;66;03m#  e.g. STFT and Feature extract\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m#       data_loader may send time-domain signal in this case\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     feats, feats_lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrontend(speech, speech_lengths)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# No frontend and no feature extract\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     feats, feats_lengths \u001b[38;5;241m=\u001b[39m speech, speech_lengths\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet2/asr/frontend/default.py:87\u001b[0m, in \u001b[0;36mDefaultFrontend.forward\u001b[0;34m(self, input, input_lengths)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor, input_lengths: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m     84\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# 1. Domain-conversion: e.g. Stft: time -> time-freq\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstft \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m         input_stft, feats_lens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_stft(\u001b[38;5;28minput\u001b[39m, input_lengths)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m         input_stft \u001b[38;5;241m=\u001b[39m ComplexTensor(\u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m], \u001b[38;5;28minput\u001b[39m[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet2/asr/frontend/default.py:122\u001b[0m, in \u001b[0;36mDefaultFrontend._compute_stft\u001b[0;34m(self, input, input_lengths)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_stft\u001b[39m(\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor, input_lengths: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m    121\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 122\u001b[0m     input_stft, feats_lens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstft(\u001b[38;5;28minput\u001b[39m, input_lengths)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m input_stft\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, input_stft\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# \"2\" refers to the real/imag parts of Complex\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/espnet2/layers/stft.py:101\u001b[0m, in \u001b[0;36mStft.forward\u001b[0;34m(self, input, ilens)\u001b[0m\n\u001b[1;32m     91\u001b[0m     stft_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     92\u001b[0m         n_fft\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_fft,\n\u001b[1;32m     93\u001b[0m         win_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwin_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m         onesided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monesided,\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     stft_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_complex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstft(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstft_kwargs)\n\u001b[1;32m    102\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(output)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/functional.py:648\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[1;32m    646\u001b[0m     extended_shape \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m-\u001b[39m signal_dim) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    647\u001b[0m     pad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_fft \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 648\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(extended_shape), [pad, pad], pad_mode)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39msignal_dim:])\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mstft(\u001b[38;5;28minput\u001b[39m, n_fft, hop_length, win_length, window,  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    651\u001b[0m                 normalized, onesided, return_complex)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 2D or 3D (batch mode) tensor with possibly 0 batch size and other non-zero dimensions for input, but got: [1, 1, 0]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import soundfile\n",
    "import librosa.display\n",
    "from IPython.display import display, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "col_names=['path', 'text', 'time', 'sr']\n",
    "preds=\"\"\n",
    "target=\"\"\n",
    "for i, chunk in enumerate(chunks_array):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df1 = df[(df['end'] >= split_sec['start'][i]*1000) & (df['end'] <= split_sec['end'][i]*1000)].copy()\n",
    "    df1=df1.sub(split_sec['start'][i]*1000)\n",
    "    if not df1.empty:\n",
    "        speech = np.array([])\n",
    "        print(df1)\n",
    "        for i, row in df1.iterrows():\n",
    "            speech = np.concatenate([speech, chunk[int(max(0,row['start'])*16):int(row['end'])*16]])\n",
    "            print(speech.shape)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    print(len(speech))\n",
    "    nbests = speech2text(speech)\n",
    "    text, *_ = nbests[0]\n",
    "# Uncomment to see details of each prediction\n",
    "#     print(f\"Input Speech: ESPNet_asr_egs/{row['path']}\")\n",
    "    # let us listen to samples\n",
    "#     display(Audio(speech, rate=rate))\n",
    "#     librosa.display.waveshow(speech, sr=rate, color=\"blue\")\n",
    "#     plt.show()\n",
    "#     print(f\"Reference text: {text_normalizer(row['text'])}\")\n",
    "#     print(f\"ASR hypothesis: {text_normalizer(text)}\")\n",
    "    output_filename = f'results/output_chunk{i}.wav'\n",
    "    sf.write(output_filename, chunk, samplerate=16000)\n",
    "    preds+=\" \"+(text_normalizer(text))\n",
    "    print(i,\"/500\")\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cc581127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SHE TRIED THIS MORNING AND ERIC WERE GOING TO  ILL PASS IT TO YOU SO THATS THE FROM THE PNL AND STAYING A SIMPLE SONG AND ITS ILL NOW TURN THE CALL OVER TO\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae72ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import CharErrorRate, WordErrorRate\n",
    "cer = CharErrorRate()\n",
    "print(\"CER:\", cer(preds, target))\n",
    "wer = WordErrorRate()\n",
    "print(\"WER:\", wer(preds, target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
